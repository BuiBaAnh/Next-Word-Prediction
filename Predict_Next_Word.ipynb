{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict Next Word.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fayqOP9dLd51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6687d035-7db8-4fee-bffa-05d420e953c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N48wL33mLrrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf8221a-1ca1-4e4a-be37-b29cf4b1b512"
      },
      "source": [
        "cd /content/drive/MyDrive/Colab\\ Notebooks/NLP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/NLP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpXVA-b7E9ox"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Activation\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VY6s5pVFQTh"
      },
      "source": [
        "SEQ_LENGTH = 40\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BQUDnt7Y9iE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a20589f-9955-4be2-fc92-b95935575d5d"
      },
      "source": [
        " !pip install nltk==3.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.4 in /usr/local/lib/python3.7/dist-packages (3.4)\n",
            "Requirement already satisfied: singledispatch in /usr/local/lib/python3.7/dist-packages (from nltk==3.4) (3.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFeYDXnnQRMm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b47f9e1-7073-45dc-9243-29cff2e61826"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "from os.path import join, dirname\n",
        "import itertools\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, TimeDistributed, Dense, LSTM, Bidirectional\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "import re\n",
        "from nltk import ngrams\n",
        "data_path = join('train.txt')\n",
        "with open(data_path, 'r') as f_r:\n",
        "    lines = f_r.read().split('\\n')\n",
        "\n",
        "print(len(lines))\n",
        "\n",
        "NGRAM = 20\n",
        "BATCH_SIZE = 64\n",
        "HIDDEN_SIZE = 256\n",
        "eng_alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "viet_alphabet = \"áàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđ\"\n",
        "digits = \"0123456789\"\n",
        "punctuations = \" _!\\\"\\',\\-\\.:;?_\\(\\)\\x00\"\n",
        "\n",
        "pattern = \"^[\" + \"\".join((eng_alphabet, viet_alphabet, digits, punctuations)) + \"]+$\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2126281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONkmAHZJD8BP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2a7ae2-c58a-4c09-c623-4256ccc35c60"
      },
      "source": [
        "# def extract_phrases(text):\n",
        "#     return ''.join(re.findall(r'\\w[\\w ]+', text))\n",
        "\n",
        "def extract_phrases(text):\n",
        "    phs = re.findall(r'\\w[\\w ]+', text)\n",
        "    phsr = []\n",
        "    for p in phs :\n",
        "        token =  text.split(p)[1]\n",
        "        if (len(token) > 0):\n",
        "            token = token[0]\n",
        "        else :\n",
        "            phsr.append(p)\n",
        "            continue                \n",
        "        if token in pattern:\n",
        "            p += token\n",
        "        phsr.append(p)\n",
        "    return [' '.join(phsr)]\n",
        "\n",
        "def gen_ngrams(words, n=NGRAM):\n",
        "    return ngrams(words.split(), n)\n",
        "    \n",
        "    \n",
        "phrases = itertools.chain.from_iterable(extract_phrases(text) for text in lines[:int(len(lines)//2)])\n",
        "phrases = [p.strip() for p in phrases if len(p.split()) > 1]\n",
        "\n",
        "list_ngrams = []\n",
        "for p in tqdm(phrases):\n",
        "    if not re.match(pattern, p.lower()):\n",
        "        continue\n",
        "    list_ngrams.append(p)\n",
        "del phrases\n",
        "list_ngrams = list(set(list_ngrams))\n",
        "print(list_ngrams[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1048493/1048493 [00:03<00:00, 330645.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['Theo ông Niệm, với đất ở nên chia gắn liền với đất sản xuất ( từ 2. 000 đến 3. 000 m2)', 'Taeniorhachis repens', 'Người dân phải đi bộ trên mái nhà nhiều tháng nay vì thang máy hư hỏng.', 'Hiện nay, các bác sĩ chẩn đoán bệnh sán lá phổi bằng kỹ thuật miễn dịch ELISA.', 'Thực tế, Nga trong trận lượt đi tại Moscow đã chơi tốt cho đến khi họ có bàn thắng nhân đôi cách biệt ở phút 51.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GLvn3UihMmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12648107-584c-4b49-ea76-2b8d03c8568e"
      },
      "source": [
        "print(len(list_ngrams))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "904459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwtmwA_rBzAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3a7eed-72bb-41fd-cbf5-3c329d55c60d"
      },
      "source": [
        "lines = ['Bộ phim lần đầu được công chiếu tại liên hoan phim Rome 2007 và sau đó được chiếu ở Fairbanks, Alaska ngày 21 tháng 9 năm 2007.']\n",
        "phrases = itertools.chain.from_iterable(extract_phrases(text) for text in lines)\n",
        "phrases = [p for p in phrases if len(p.split()) > 1]\n",
        "phrases\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bộ phim lần đầu được công chiếu tại liên hoan phim Rome 2007 và sau đó được chiếu ở Fairbanks, Alaska ngày 21 tháng 9 năm 2007.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwCGYpZ-FVIW"
      },
      "source": [
        "file = open('train.txt', encoding = 'utf8')\n",
        "raw_text = file.read()\n",
        "# raw_text = raw_text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnHxR6RqFbj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d6c392-bb2d-46ff-8c65-fd48c6ec6626"
      },
      "source": [
        "import string \n",
        "\n",
        "eng_alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "viet_alphabet = \"áàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđ\"\n",
        "digits = \"0123456789\"\n",
        "punctuations = \" _!\\\"\\',\\-\\.:;?_\\(\\)\\x00\"\n",
        "\n",
        "pattern = \"^[\" + \"\".join((eng_alphabet, viet_alphabet, digits, punctuations)) + \"]+$\"\n",
        "\n",
        "accented_chars_vietnamese = [\n",
        "    'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ',\n",
        "    'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ',\n",
        "    'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ',\n",
        "    'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự',\n",
        "    'í', 'ì', 'ỉ', 'ĩ', 'ị',\n",
        "    'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ',\n",
        "    'đ',\n",
        "]\n",
        "accented_chars_vietnamese.extend([c.upper() for c in accented_chars_vietnamese])\n",
        "chars = list(('\\x00 _' + string.ascii_letters + string.digits\n",
        "                 + ''.join(accented_chars_vietnamese) + string.punctuation))\n",
        "\n",
        "\n",
        "print(chars)\n",
        "VOCABULARY = len(chars)\n",
        "\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNzlxHaaFi5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c007082a-6ba1-4427-f564-ff16e4b3dfc2"
      },
      "source": [
        "text_length = len(raw_text)\n",
        "char_length = len(chars)\n",
        "print(\"Text length = \" + str(text_length))\n",
        "print(\"No. of characters = \" + str(char_length))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text length = 199087994\n",
            "No. of characters = 231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN63sP2iaD6Z"
      },
      "source": [
        "def make_noise(text):\n",
        "    x = []\n",
        "    y = []\n",
        "    text = ' '*(SEQ_LENGTH - 1) + text\n",
        "    if (len(text) <= SEQ_LENGTH):\n",
        "        text = ' '*(SEQ_LENGTH - len(text)) + text\n",
        "        x_t = text[: SEQ_LENGTH -1]\n",
        "        x_tt = [char_to_int[char] for char in x_t]\n",
        "        x.append(x_tt)\n",
        "        y.append(char_to_int[text[SEQ_LENGTH -1]])\n",
        "    else :\n",
        "        if (len(text)>=1500):\n",
        "            text = text[:1500]\n",
        "        for i in range(len(text) - SEQ_LENGTH):\n",
        "            x_t = text[i :i + SEQ_LENGTH - 1]\n",
        "            x_tt = [char_to_int[char] for char in x_t]\n",
        "            x.append(x_tt)\n",
        "            y.append(char_to_int[text[i + SEQ_LENGTH - 1]])\n",
        "    return x,y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkKJu2vzZVop"
      },
      "source": [
        "import numpy as np\n",
        "def generate_data(data, batch_size=64):\n",
        "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "    cur_index = 0\n",
        "    while True:\n",
        "        x = []\n",
        "        y = []\n",
        "        for i in range(batch_size):\n",
        "            w, l = make_noise(data[cur_index])     \n",
        "            x += w\n",
        "            y += l\n",
        "            cur_index += 1\n",
        "\n",
        "            if cur_index > len(data) - 1:\n",
        "                cur_index = 0\n",
        "        # length = len(x)\n",
        "        x = np.array(x)\n",
        "        x = np. reshape(x, (x.shape[0], x.shape[1], 1))\n",
        "        x = x/float(VOCABULARY)\n",
        "\n",
        "        y = np.array(y)\n",
        "        y = np_utils.to_categorical(y, num_classes=VOCABULARY)\n",
        "        yield x, y\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpsg_jG8Ze1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c06dedc-f1c7-47c2-e4c7-cd84470b2e48"
      },
      "source": [
        "train_data, valid_data = train_test_split(list_ngrams, test_size=0.2)\n",
        "print(len(train_data))\n",
        "train_generator = generate_data(train_data, batch_size=BATCH_SIZE)\n",
        "validation_generator = generate_data(valid_data, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "723567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbjpEQl1FSOY"
      },
      "source": [
        "def buildmodel(VOCABULARY):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(256, input_shape = (SEQ_LENGTH, 1), return_sequences = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(256))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(VOCABULARY, activation = 'softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X61_Q5FzHl_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd55bddf-4746-4f58-ad46-20318506874f"
      },
      "source": [
        "model = buildmodel(VOCABULARY)\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join('trained_model/model1_{val_loss:.4f}_{val_accuracy:.4f}.hdf5'),\n",
        "                               save_best_only=True, verbose=1)\n",
        "early = EarlyStopping(patience=2, verbose=1)\n",
        "BATCH_SIZE = 64\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=len(train_data)//BATCH_SIZE, epochs=5,\n",
        "                    validation_data=validation_generator, validation_steps=len(valid_data)//BATCH_SIZE,\n",
        "                    callbacks=[checkpoint, early])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "11305/11305 [==============================] - 10055s 889ms/step - loss: 2.6046 - accuracy: 0.3422 - val_loss: 1.8786 - val_accuracy: 0.4877\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.87857, saving model to trained_model/model1_1.8786_0.4877.hdf5\n",
            "Epoch 2/5\n",
            "11305/11305 [==============================] - 10031s 887ms/step - loss: 1.9368 - accuracy: 0.4739 - val_loss: 1.7490 - val_accuracy: 0.5204\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.87857 to 1.74896, saving model to trained_model/model1_1.7490_0.5204.hdf5\n",
            "Epoch 3/5\n",
            "11305/11305 [==============================] - 10031s 887ms/step - loss: 1.8386 - accuracy: 0.5000 - val_loss: 1.6879 - val_accuracy: 0.5369\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.74896 to 1.68793, saving model to trained_model/model1_1.6879_0.5369.hdf5\n",
            "Epoch 4/5\n",
            " 1610/11305 [===>..........................] - ETA: 2:12:10 - loss: 1.7956 - accuracy: 0.5115"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDu6ar_Gvs-U"
      },
      "source": [
        "filename = 'trained_model/model1_1.9452_0.4711.hdf5'\n",
        "model = buildmodel(VOCABULARY)\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2V9zKuPvxqc"
      },
      "source": [
        "initial_text = 'Trong chiến thắng bất ngờ của đội tuyển Việt Nam, '# we sat here we two and we said how we wish we had something to do.'\n",
        "print(len(initial_text))\n",
        "initial_text = [char_to_int[c] for c in initial_text[-50:]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__4MaK9gv3GS"
      },
      "source": [
        "GENERATED_LENGTH = 1000\n",
        "test_text = initial_text\n",
        "generated_text = []\n",
        "\n",
        "for i in range(100):\n",
        "    X = np.reshape(test_text, (1, SEQ_LENGTH, 1))\n",
        "    next_character = model.predict(X/float(VOCABULARY))\n",
        "    index = np.argmax(next_character)\n",
        "    generated_text.append(int_to_char[index])\n",
        "    test_text.append(index)\n",
        "    test_text = test_text[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8v9zPKsZN3R"
      },
      "source": [
        "generated_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz4kfZNMY-YP"
      },
      "source": [
        "print(''.join(generated_text))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}